{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Del 14: Procesiranje velikih datasetov v pandas-u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma = pd.read_csv('data/MoMAExhibitions1929to1989.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the memory usage of a Pandas DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Internal Representation of a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pandas_dataframe_blocks.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma._data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Memory Footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numbers (int, float) and other fixed-size objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.Series([123] * 1_000_000, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: moma dataset - float clomun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "34558*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma['ExhibitionID'].nbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Object Columns (arbitrarily-sized objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/numpy_vs_python.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bytes = moma.size*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_megabytes = total_bytes/(1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total bytes:', total_bytes)\n",
    "print('Total megabytes:', total_megabytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **index** : *bool, default True* --> \n",
    "Specifies whether to include the memory usage of the DataFrame’s index in returned Series. If index=True, the memory usage of the index is the first item in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma.memory_usage(deep=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma[\"ExhibitionTitle\"].memory_usage()/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma[\"ExhibitionTitle\"].memory_usage(deep=True)/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "(sum([sys.getsizeof(s) for s in moma[\"ExhibitionTitle\"]]) + moma[\"ExhibitionTitle\"].memory_usage())/1024/1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting memory usage by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage_by_type(df, types=[]):\n",
    "    if not types:\n",
    "        types = []\n",
    "        for column in df.columns:\n",
    "            if hasattr(df[column], 'cat'):\n",
    "                types.append('category')\n",
    "            else:\n",
    "                types.append(df[column].dtype)\n",
    "        types = list(set(types))\n",
    "    total = 0\n",
    "    for dtype in types:\n",
    "        selected_dtype = df.select_dtypes(include=[dtype])\n",
    "        num_of_columns = len(selected_dtype.columns)\n",
    "        mean_usage_b = selected_dtype.memory_usage(deep=True, index=False).mean()\n",
    "        mean_usage_mb = mean_usage_b / 1024 ** 2\n",
    "        sum_usage_b = selected_dtype.memory_usage(deep=True, index=False).sum()\n",
    "        sum_usage_mb = sum_usage_b / 1024 ** 2\n",
    "        print(f\"Average memory usage: {round(mean_usage_mb, 3)} MB and total: {round(sum_usage_mb, 3)} MB for {num_of_columns}x {dtype} columns.\")\n",
    "        total += sum_usage_mb\n",
    "        \n",
    "    print('----------------------')\n",
    "    print(f'Total memory usage: {round(total, 3)} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_usage_by_type(moma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Dataframe Memory Footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma_sample = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\", nrows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da dobimo imena vseh stolpcev\n",
    "moma_sample.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['ExhibitionID', 'ExhibitionNumber', 'ExhibitionBeginDate', \n",
    "             'ExhibitionEndDate', 'DisplayName', 'Institution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma_small = pd.read_csv('data/MoMAExhibitions1929to1989.csv',\n",
    "                    usecols=keep_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_usage_by_type(moma_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Numeric Columns with Smaller Subtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>memory usage</th>\n",
    "<th>float</th>\n",
    "<th>int</th>\n",
    "<th>uint</th>\n",
    "<th>datetime</th>\n",
    "<th>bool</th>\n",
    "<th>object</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>1 bytes</td>\n",
    "<td></td>\n",
    "<td>int8</td>\n",
    "<td>uint8</td>\n",
    "<td></td>\n",
    "<td>bool</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2 bytes</td>\n",
    "<td>float16</td>\n",
    "<td>int16</td>\n",
    "<td>uint16</td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4 bytes</td>\n",
    "<td>float32</td>\n",
    "<td>int32</td>\n",
    "<td>uint32</td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>8 bytes</td>\n",
    "<td>float64</td>\n",
    "<td>int64</td>\n",
    "<td>uint64</td>\n",
    "<td>datetime64</td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>variable</td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "<td>object</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_types = [\"int8\", \"int16\", \"int32\", \"int64\"]\n",
    "for it in int_types:\n",
    "    print(np.iinfo(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.iinfo('int8').min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.iinfo('int8').max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integer Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error\n",
    "#moma['ConstituentID'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma.select_dtypes(include=['float']).isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\")\n",
    "moma.select_dtypes(include=['float']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_usage_by_type(moma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert to best integer subtype\n",
    "def convert_col_to_best_int_subtype(df, columns=[]):\n",
    "    for column in columns:\n",
    "        col_max = df[column].max()\n",
    "        col_min = df[column].min()\n",
    "        if col_max < np.iinfo('int8').max and col_min > np.iinfo('int8').min:\n",
    "            print(f\"Column {column} converted to int8.\")\n",
    "            df[column] = df[column].astype('int8')\n",
    "        elif col_max <  np.iinfo(\"int16\").max and col_min > np.iinfo(\"int16\").min:\n",
    "            print(f\"Column {column} converted to int16.\")\n",
    "            df[column] = df[column].astype(\"int16\")\n",
    "        elif col_max <  np.iinfo(\"int32\").max and col_min > np.iinfo(\"int32\").min:\n",
    "            print(f\"Column {column} converted to int32.\")\n",
    "            df[column] = df[column].astype(\"int32\")\n",
    "        elif col_max <  np.iinfo(\"int64\").max and col_min > np.iinfo(\"int64\").min:\n",
    "            print(f\"Column {column} converted to int64.\")\n",
    "            df[column] = df[column].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_col_to_best_int_subtype(moma, ['ExhibitionSortOrder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_usage_by_type(moma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Float Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(moma) * 2 + 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma['ExhibitionSortOrder'].memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the dataframe to the original CSV\n",
    "moma = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\")\n",
    "\n",
    "moma['ExhibitionSortOrder'] = moma['ExhibitionSortOrder'].astype('int')\n",
    "moma['ExhibitionSortOrder'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the dataframe to the original CSV\n",
    "moma = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\")\n",
    "\n",
    "moma['ExhibitionSortOrder'] = moma['ExhibitionSortOrder'].astype('int')\n",
    "moma['ExhibitionSortOrder'] = pd.to_numeric(moma['ExhibitionSortOrder'], \n",
    "                                           downcast='integer')\n",
    "\n",
    "moma['ExhibitionSortOrder'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the dataframe to the original CSV\n",
    "moma = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\")\n",
    "moma['ExhibitionSortOrder'] = pd.to_numeric(moma['ExhibitionSortOrder'], \n",
    "                                            downcast='integer')\n",
    "\n",
    "moma['ExhibitionSortOrder'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\")\n",
    "get_memory_usage_by_type(moma)\n",
    "# convert int columns\n",
    "convert_col_to_best_int_subtype(moma, ['ExhibitionSortOrder'])\n",
    "# convert folat columns\n",
    "float_cols = moma.select_dtypes(include=['float'])\n",
    "for col in float_cols.columns:\n",
    "    moma[col] = pd.to_numeric(moma[col], downcast='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_usage_by_type(moma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting To DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma['ExhibitionEndDate'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma['ExhibitionEndDate'] = pd.to_datetime(moma['ExhibitionEndDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format parameter -> faster \n",
    "moma['ExhibitionBeginDate'] = pd.to_datetime(moma['ExhibitionBeginDate'], format='%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_usage_by_type(moma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma['ConstituentType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma['ConstituentType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma['ConstituentType'].memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma['ConstituentType'] = moma['ConstituentType'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_usage_by_type(moma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma['ConstituentType'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_col = moma.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in obj_col.columns:\n",
    "    num_unique_values = len(moma[col].unique())\n",
    "    num_total_values = len(moma[col])\n",
    "    \n",
    "    if num_unique_values / num_total_values < 0.5:\n",
    "        moma[col] = moma[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_usage_by_type(moma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html\n",
    "- https://pythonspeed.com/articles/pandas-load-less-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  Selecting Types While Reading the Data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma_sample = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\", nrows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da dobimo imena vseh stolpcev\n",
    "moma_sample.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['ExhibitionID', 'ExhibitionNumber', 'ExhibitionBeginDate', \n",
    "             'ExhibitionEndDate', 'ExhibitionSortOrder', 'ExhibitionRole', \n",
    "             'ConstituentType', 'DisplayName', 'Institution', 'Nationality', \n",
    "             'Gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_types = {'ExhibitionID': np.float32, \n",
    "             'ExhibitionNumber': 'category',\n",
    "             'ExhibitionSortOrder': np.float16, \n",
    "             'ExhibitionRole': 'category', \n",
    "             'ConstituentType' : 'category', \n",
    "             'DisplayName' : 'category', \n",
    "             'Institution': 'category',  \n",
    "             'Nationality' : 'category', \n",
    "             'Gender': 'category'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = [\"ExhibitionBeginDate\", \"ExhibitionEndDate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma = pd.read_csv('data/MoMAExhibitions1929to1989.csv',\n",
    "                    usecols=keep_cols,\n",
    "                    parse_dates=date_cols,\n",
    "                    dtype=col_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory_usage_by_type(moma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Dataframes in Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/processing_chunks_overview.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_iter = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\", chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(chunk_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunk_iter:\n",
    "    print(len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator object that reads in 250-row chunks from \"moma.csv\".\n",
    "chunk_iter = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\", chunksize=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each chunk, retrieve the memory footprint in megabytes and append it to the list memory_footprints.\n",
    "memory_footprints = []\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    memory_footprints.append(chunk.memory_usage(deep=True).sum()/(1024*1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display a histogram of the values in memory_footprints using pyplot.hist()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(memory_footprints)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Across Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator object that reads in 250-row chunks from \"moma.csv\".\n",
    "chunk_iter = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\", chunksize=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each chunk, retrieve the number of rows and add it to num_rows.\n",
    "num_rows = 0\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    num_rows += len(chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/process_chunks_count.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_list = [pd.Series([1,2]), pd.Series([2,3])]\n",
    "\n",
    "pd.concat(series_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifespans = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = { 'ConstituentBeginDate': 'float',\n",
    "          'ConstituentEndDate': 'float'}\n",
    "\n",
    "chunk_iter = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\", \n",
    "                         chunksize=250,\n",
    "                        dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunk_iter:\n",
    "    diff = chunk['ConstituentEndDate'] - chunk['ConstituentBeginDate']\n",
    "    lifespans.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifespans_dist = pd.concat(lifespans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifespans_dist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "lifespans = []\n",
    "\n",
    "chunk_iter = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\", chunksize=250, \n",
    "                         dtype={\"ConstituentBeginDate\": \"float\", \"ConstituentEndDate\": \"float\"})\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    lifespans.append(chunk['ConstituentEndDate'] - chunk['ConstituentBeginDate'])\n",
    "\n",
    "lifespans_dist = pd.concat(lifespans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "lifespans = []\n",
    "\n",
    "chunk_iter = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\", chunksize=500, \n",
    "                         dtype={\"ConstituentBeginDate\": \"float\", \"ConstituentEndDate\": \"float\"},  \n",
    "                         usecols=['ConstituentBeginDate', 'ConstituentEndDate'])\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    lifespans.append(chunk['ConstituentEndDate'] - chunk['ConstituentBeginDate'])\n",
    "    \n",
    "lifespans_dist = pd.concat(lifespans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Unique Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/processing_chunks_value_counts.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_iter = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\",\n",
    "                         chunksize=250, usecols=['Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_vc = []\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    chunk_vc = chunk['Gender'].value_counts()\n",
    "    overall_vc.append(chunk_vc)\n",
    "    \n",
    "combined_vc = pd.concat(overall_vc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_vc.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Chunks Using GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_iter = pd.read_csv(\"data/MoMAExhibitions1929to1989.csv\",\n",
    "                         chunksize=250, usecols=['Gender'])\n",
    "\n",
    "overall_vc = []\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    chunk_vc = chunk['Gender'].value_counts()\n",
    "    overall_vc.append(chunk_vc)\n",
    "    \n",
    "combined_vc = pd.concat(overall_vc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vc = combined_vc.groupby(combined_vc.index).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizing big files with Pandas and SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/moma.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moma_iter = pd.read_csv('data/moma.csv', chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in moma_iter:\n",
    "    chunk.to_sql('exhibitions', conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Primarily in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/moma.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '''SELECT exhibitionid, count(*) AS counts \n",
    "    from exhibitions \n",
    "    GROUP BY exhibitionid \n",
    "    ORDER BY counts desc;'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eid_counts = pd.read_sql(q, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eid_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Primarily in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/moma.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'SELECT exhibitionid FROM exhibitions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eid_counts = pd.read_sql(q, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eid_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eid_pandas_counts = eid_counts['ExhibitionID'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eid_pandas_counts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in SQL Results Using Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data/moma.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "q = 'select exhibitionid from exhibitions;'\n",
    "chunk_iter = pd.read_sql(q, conn, chunksize=100)\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    eid_pandas_counts = eid_counts['ExhibitionID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "q = 'select exhibitionid from exhibitions;'\n",
    "chunk_iter = pd.read_sql(q, conn, chunksize=1000)\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    eid_pandas_counts = eid_counts['ExhibitionID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "q = 'select exhibitionid from exhibitions;'\n",
    "chunk_iter = pd.read_sql(q, conn, chunksize=10000)\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    eid_pandas_counts = eid_counts['ExhibitionID'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vaja: Primer analize velikega dataseta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_iter = pd.read_csv('data/crunchbase-investments.csv', \n",
    "                         chunksize=5000, \n",
    "                        encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_list = []\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    print(chunk.memory_usage(deep=True).sum() / (1024 * 1024))\n",
    "    mv_list.append(chunk.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_mv_vc = pd.concat(mv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_mv_vc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_combined_mv_vc = combined_mv_vc.groupby(combined_mv_vc.index).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_combined_mv_vc.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_iter = pd.read_csv('data/crunchbase-investments.csv', \n",
    "                         chunksize=5000, \n",
    "                        encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "series_memory_fp = pd.Series(dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunk_iter:\n",
    "    if counter == 0:\n",
    "        series_memory_fp = chunk.memory_usage(deep=True)\n",
    "    else: \n",
    "        series_memory_fp += chunk.memory_usage(deep=True)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop memory footprint calculation for the index.\n",
    "series_memory_fp = series_memory_fp.drop('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_memory_fp.sum() / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_iter = pd.read_csv('data/crunchbase-investments.csv', \n",
    "                         chunksize=5000, \n",
    "                        encoding='ISO-8859-1')\n",
    "total_rows = 0\n",
    "for chunk in chunk_iter:\n",
    "    total_rows += len(chunk)\n",
    "\n",
    "print(total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_combined_mv_vc.sort_values()/total_rows*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns representing URL's or containing way too many missing values (>90% missing)\n",
    "drop_cols = ['investor_permalink', 'company_permalink', \n",
    "             'investor_category_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = chunk.columns.drop(drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key: Column name, Value: List of types\n",
    "col_types = {}\n",
    "chunk_iter = pd.read_csv('data/crunchbase-investments.csv', \n",
    "                         chunksize=5000, \n",
    "                         encoding='ISO-8859-1', \n",
    "                         usecols=keep_cols)\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    for col in chunk.columns:\n",
    "        if col not in col_types:\n",
    "            col_types[col] = [str(chunk.dtypes[col])]\n",
    "        else:\n",
    "            col_types[col].append(str(chunk.dtypes[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_col_types = {}\n",
    "for k,v in col_types.items():\n",
    "    uniq_col_types[k] = set(col_types[k])\n",
    "uniq_col_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('data/crunchbase.db')\n",
    "chunk_iter = pd.read_csv('data/crunchbase-investments.csv', \n",
    "                         chunksize=5000, \n",
    "                         encoding='ISO-8859-1',\n",
    "                         usecols=keep_cols)\n",
    "\n",
    "for chunk in chunk_iter:\n",
    "    chunk.to_sql(\"investments\", conn, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previmo z podatke\n",
    "q = \"SELECT * FROM investments LIMIT 5;\"\n",
    "data_5 = pd.read_sql(q, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '''SELECT company_category_code, count(*) AS counts \n",
    "    from investments \n",
    "    GROUP BY company_category_code \n",
    "    ORDER BY counts desc;'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_sql(q, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head(10).plot(kind='bar', \n",
    "          x='company_category_code', \n",
    "          y='counts', \n",
    "          legend=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More file formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Big Data file formats](https://luminousmen.com/post/big-data-file-formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Parquet](https://parquet.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launched in 2013, Parquet was developed by Cloudera and Twitter to serve as a column-based storage format, optimized for work with multi-column datasets. Because data is stored by columns, it can be highly compressed (compression algorithms perform better on data with low information entropy which is usually contained in columns) and splittable. The developers of the format claim that this storage format is ideal for Big Data problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Avro](https://avro.apache.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Avro was released by the Hadoop working group in 2009. It is a row-based format that is highly splittable. It also described as a data serialization system similar to Java Serialization. The schema is stored in JSON format while the data is stored in binary format, minimizing file size and maximizing efficiency. Avro has robust support for schema evolution by managing added fields, missing fields, and fields that have changed. This allows old software to read the new data and new software to read the old data — a critical feature if your data has the potential to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Feather](https://github.com/wesm/feather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feather provides binary columnar serialization for data frames. It is designed to make reading and writing data frames efficient, and to make sharing data across data analysis languages easy. Feather uses the Apache Arrow columnar memory specification to represent binary data on disk. This makes read and write operations very fast. This is particularly important for encoding null/NA values and variable-length types like UTF8 strings.\n",
    "\n",
    "Feather is a part of the broader Apache Arrow project. Feather defines its own simplified schemas and metadata for on-disk representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [HDF5](https://portal.hdfgroup.org/display/knowledge/What+is+HDF5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 is a unique technology suite that makes possible the management of extremely large and complex data collections.\n",
    "\n",
    "The HDF5 technology suite is designed to organize, store, discover, access, analyze, share, and preserve diverse, complex data in continuously evolving heterogeneous computing and storage environments.\n",
    "\n",
    "HDF5 supports all types of data stored digitally, regardless of origin or size. Petabytes of remote sensing data collected by satellites, terabytes of computational results from nuclear testing models, and megabytes of high-resolution MRI brain scans are stored in HDF5 files, together with metadata necessary for efficient data sharing, processing, visualization, and archiving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c6c10c76ac698377918c67c76da7a324dc5377550a96cf6b34f2c5eed606983"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
